{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers\n",
    "import time\n",
    "! pip install sentencepiece\n",
    "\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer, BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "import textwrap\n",
    "\n",
    "import torch\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "import os\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\", model = \"facebook/bart-large-xsum\")\n",
    "\n",
    "\n",
    "print(\"Time for import:\")\n",
    "print(time.perf_counter() - t1)\n",
    "\n",
    "## Setting to use the 0th GPU\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "\n",
    "def getText(url):\n",
    "    scraped_data = urllib.request.urlopen(url)\n",
    "    article = scraped_data.read()\n",
    "\n",
    "    parsed_article = bs.BeautifulSoup(article, \"lxml\")\n",
    "\n",
    "    paragraphs = parsed_article.find_all(\"p\")\n",
    "\n",
    "    article_text = \"\"\n",
    "\n",
    "    for p in paragraphs:\n",
    "        article_text += p.text\n",
    "\n",
    "    article_text = re.sub(r\"\\[[0-9]*\\]\", \" \", article_text)\n",
    "    article_text = re.sub(r\"\\s+\", \" \", article_text)\n",
    "    return article_text\n",
    "\n",
    "\n",
    "def getSummary(url):\n",
    "    # t1 = time.perf_counter()\n",
    "    text = getText(url)\n",
    "    # text = text.split(\".\")\n",
    "\n",
    "    # t1 = time.perf_counter()\n",
    "\n",
    "    # model_name = 'google/pegasus-xsum'\n",
    "\n",
    "    # device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "    # # batch = tokenizer(text, truncation=True, padding='longest', return_tensors=\"pt\").to(device)\n",
    "    # batch = tokenizer(text,max_length = 128, truncation=True, padding='longest', return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # translated = model.generate(**batch)\n",
    "\n",
    "    # summary_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "\n",
    "    # ---------------- for bart normal-------------------------\n",
    "    summary_text = summarizer(text, min_length = 60, max_length = 144, truncation = True, do_sample=False)\n",
    "\n",
    "    # ---------------- for bart tuned --------------------------\n",
    "    \n",
    "    # tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "    # model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "    # print(text)\n",
    "    # max_input_length = 1024\n",
    "    # input_tokens = tokenizer.batch_encode_plus(text, return_tensors = 'pt', max_length = 1024, padding = True, truncation=True)['input_ids']\n",
    "    # # model_inputs = tokenizer(text, max_length=max_input_length, truncation=True, return_tensors='pt')['input_ids']\n",
    "\n",
    "    # encoded_ids = model.generate(input_tokens, num_beams = 4, length_penalty = 2.0, max_length = 150, min_length = 50, no_repeat_ngram_size = 3)\n",
    "\n",
    "    # summary_text = tokenizer.decode(encoded_ids.squeeze(), skip_special_tokens = True)\n",
    "    return summary_text\n",
    "\n",
    "\n",
    "def main():\n",
    "    url = \"https://simple.wikipedia.org/wiki/Artificial_intelligence\"\n",
    "    urls = [\n",
    "        \"https://simple.wikipedia.org/wiki/Artificial_intelligence\",\n",
    "        \"https://simple.wikipedia.org/wiki/Commodore_Nutt\",\n",
    "        \"https://simple.wikipedia.org/wiki/Economics\",\n",
    "        \"https://simple.wikipedia.org/wiki/Health\",\n",
    "        \"https://simple.wikipedia.org/wiki/Anatomy\",\n",
    "        \"https://simple.wikipedia.org/wiki/Human_rights\",\n",
    "        \"https://simple.wikipedia.org/wiki/Hinduism\",\n",
    "        \"https://simple.wikipedia.org/wiki/Movie\",\n",
    "    ]\n",
    "\n",
    "    for url in urls:\n",
    "      summary = getSummary(url)[0]\n",
    "\n",
    "      print(\"\\nSummary:\\n\")\n",
    "      print(summary)\n",
    "\n",
    "\n",
    "print(\"Time for compilation:\")\n",
    "print(time.perf_counter() - t1)\n",
    "\n",
    "if _name_ == \"_main_\":\n",
    "    t1 = time.perf_counter()\n",
    "    main()\n",
    "    print(\"Time for execution of main:\")\n",
    "    print(time.perf_counter() - t1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
